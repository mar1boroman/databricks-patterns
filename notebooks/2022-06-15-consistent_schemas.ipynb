{"cells":[{"cell_type":"markdown","source":["## My use case for maintaining consistent schemas across dataframes\n\nOne of the ways we migrate ETL jobs from tools like Informatica/Datastage to pyspark code is by converting every stage/transformation we have in the ETL job to a dataframe in pyspark code. While every stage/transformation in the ETL job has its own unique value, most stages/transformations retain the schema (unless explicitly changed). To replicate this logic in pyspark code becomes tedious as manual intervention is required to ensure consistency of datatypes, scales and precisions while we derive subsequent dataframes from the source dataframes. The notebook shown below addresses the issue by providing a simple function which takes the source dataframe and casts the target dataframe into the same datatype."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"058499d3-e661-4373-9217-954795e14ff6"}}},{"cell_type":"markdown","source":["## Assumptions\n\nI have made the following assumptions, since this is a simple example.\n\n1. Your source dataframe (the one with a defined schema) and the target dataframe have the same column names\n2. This is a simple example, you could extend the logic based on your specific requirement"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9fe6684-8cb5-4ac0-9fd6-09f4849bb789"}}},{"cell_type":"markdown","source":["## Imports & setting up initial data frame\n\nLets set up the dataframes `init_df_with_schema` and `new_df_without_schema` & initialize a dataframe with a specified schema denoted by `init_schema`.\nNote that `new_df_without_schema` does not have a specific schema, and the columns in `new_df_without_schema` need to be converted to appropriate datatypes to match the column datatypes in `init_df_with_schema`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ee3a15-d881-4c5b-b7f0-ac807750e737"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, TimestampType\nfrom datetime import datetime\n\ndata1 = [\n    [1, \"John1\", \"Doe1\", datetime.now(), 31.5],\n    [2, \"John2\", \"Doe2\", datetime.now(), 37.5],\n    [3, \"John3\", \"Doe3\", datetime.now(), 62.5],\n    [4, \"John4\", \"Doe4\", datetime.now(), 74.5]\n]\n\ndata2 = [\n    ['1', \"Jane1\", \"Doe1\", '10:05:00.00', '31.5'],\n    ['2', \"Jane2\", \"Doe2\", '13:10:00.12', '37.5'],\n    ['3', \"Jane3\", \"Doe3\", '18:30:00.30', '62.5'],\n    ['4', \"Jane4\", \"Doe4\", '21:45:00.44', '74.5']\n]\n\ninit_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"FirstName\", StringType(), True),\n    StructField(\"LastName\", StringType(), True),\n    StructField(\"SnapshotTime\", TimestampType(), True),\n    StructField(\"Metric\", FloatType(), True),\n])\n\ninit_df_with_schema = spark.createDataFrame(data=data1, schema=init_schema)\ninit_df_with_schema.schema\n\nnew_df_without_schema = spark.createDataFrame(data2,[\"id\", \"FirstName\", \"LastName\", \"SnapshotTime\", \"Metric\"])\nnew_df_without_schema.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"9e78230d-3c36-4a78-8095-e0bccd15d408"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Using the Initial DataFrame to cast the columns of the New DataFrame\n\nWe would use a simple for loop to iterate through the initial dataframes datatypes and column names and cast the new/target dataframe columns with the appropriate datatypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd8657c2-243d-4893-9213-d7dcec74be5f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfor metadata in init_df_with_schema.dtypes:\n    new_df_without_schema = new_df_without_schema.withColumn(metadata[0], col(metadata[0]).cast(metadata[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb89ed5b-744a-4554-a4f3-e51770bab375"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Testing \n\nCompare the schemas using `dataframe.schema`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac5c874d-bbd7-4cbf-94eb-c585d3c366f4"}}},{"cell_type":"code","source":["if(new_df_without_schema.schema == init_df_with_schema.schema):\n    print(\"Schema matches!\")\nelse:\n    print(\"Schema does not match\")\n    print(new_df_without_schema.schema)\n    print('***************************')\n    print(init_df_with_schema.schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"006253b9-b28a-4bc0-ae60-75e7ef8d3b14"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2022-06-15-consistent_schemas","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1787786643574782}},"nbformat":4,"nbformat_minor":0}
